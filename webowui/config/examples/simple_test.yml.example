# ============================================================================
# SIMPLE TEST CONFIGURATION
# ============================================================================
#
# This is a minimal example for testing and learning web-to-openwebui.
# It demonstrates the bare minimum configuration required to scrape a site.
#
# USAGE:
#   1. Copy this file: cp simple_test.yml.example mysite.yaml
#   2. Edit the URLs and patterns below
#   3. Test: webowui validate --site mysite
#   4. Scrape: webowui scrape --site mysite
#
# For more advanced features, see:
#   - mediawiki.yml.example (optimized for MediaWiki sites)
#   - example_site.yml.example (comprehensive reference)
#
# ============================================================================

# ----------------------------------------------------------------------------
# SITE INFORMATION (Required)
# ----------------------------------------------------------------------------
site:
  # Internal identifier - lowercase, no spaces, no special characters
  # This is how you reference the site in CLI commands
  name: "simple_test"
  
  # Human-readable display name (appears in logs and reports)
  display_name: "Simple Test Site"
  
  # Base URL - the root domain of the site you're scraping
  # All URLs must start with this base URL
  base_url: "https://example.com"
  
  # Starting URLs - where the scraper begins
  # These must be full URLs (not relative paths)
  start_urls:
    - "https://example.com/page"

# ----------------------------------------------------------------------------
# CRAWLING STRATEGY (Required)
# ----------------------------------------------------------------------------
strategy:
  # Strategy type - how the scraper follows links
  # Options: "recursive" (follow matching links), "selective" (explicit URLs only)
  type: "recursive"
  
  # Maximum depth - how many link levels to follow from start URLs
  # depth=0: Only scrape start_urls
  # depth=1: Scrape start_urls + direct links from them
  # depth=2: Go one level deeper, etc.
  # Start with 1 for testing!
  max_depth: 1
  
  # Follow patterns - regex patterns matching URLs to scrape
  # Use regex escape: \. for literal dots
  follow_patterns:
    - "^https://example\\.com/.*"
  
  # Exclude patterns - URLs matching these will NOT be scraped
  # Even if they match follow_patterns
  exclude_patterns:
    - ".*\\.pdf$"          # Skip PDF files
    - ".*\\.jpg$"          # Skip images
    - ".*\\?.*"            # Skip URLs with query parameters

# ----------------------------------------------------------------------------
# RATE LIMITING (Required)
# ----------------------------------------------------------------------------
rate_limit:
  # Requests per second - be respectful to the server!
  # 1-2 req/sec is safe for most public sites
  requests_per_second: 1
  
  # Delay between requests (in seconds)
  # Additional throttling on top of requests_per_second
  delay: 0.5

# ----------------------------------------------------------------------------
# CONTENT EXTRACTION (Optional - uses sensible defaults)
# ----------------------------------------------------------------------------
extraction:
  # CSS selector for main content
  # "body" = extract everything (default)
  content_selector: "body"
  
  # Selectors to remove before extraction (list of CSS selectors)
  # Comment out to disable
  # remove_selectors:
  #   - "nav"
  #   - "header"
  #   - "footer"

# ----------------------------------------------------------------------------
# FILTERS (Optional - uses sensible defaults)
# ----------------------------------------------------------------------------
filters:
  # Minimum content length (characters) - too short = probably navigation
  min_content_length: 100
  
  # Maximum content length (characters) - too long = potential issue
  max_content_length: 500000

# ----------------------------------------------------------------------------
# CONTENT CLEANING (Optional)
# ----------------------------------------------------------------------------
cleaning:
  # Cleaning profile to use
  # "none" = no cleaning (raw markdown from crawl4ai)
  # "mediawiki" = remove wiki boilerplate
  # "fandomwiki" = remove Fandom-specific elements
  profile: "none"
  
  # Profile-specific configuration (if needed)
  # config:
  #   filter_dead_links: false

# ----------------------------------------------------------------------------
# OPENWEBUI INTEGRATION (Optional - required for upload)
# ----------------------------------------------------------------------------
# Uncomment to enable automatic uploads
# openwebui:
#   # Knowledge base name in OpenWebUI
#   knowledge_name: "Simple Test"
#   
#   # Auto-upload after each scrape
#   auto_upload: false
#   
#   # Preserve files when removed from scrape (don't delete from OpenWebUI)
#   preserve_deleted_files: false

# ----------------------------------------------------------------------------
# RETENTION (Optional - disabled by default)
# ----------------------------------------------------------------------------
# Uncomment to enable automatic backup cleanup
# retention:
#   enabled: false
#   keep_backups: 2
#   auto_cleanup: true

# ----------------------------------------------------------------------------
# SCHEDULING (Optional - disabled by default)
# ----------------------------------------------------------------------------
# Uncomment to enable scheduled scraping (Docker daemon mode)
# schedule:
#   enabled: false
#   type: "cron"
#   cron: "0 2 * * *"  # 2 AM daily
#   timezone: "America/Los_Angeles"

# ============================================================================
# THAT'S IT! This is the minimum configuration needed.
# 
# NEXT STEPS:
#   1. Save this file with .yaml extension (not .yml.example)
#   2. Edit the base_url and start_urls above
#   3. Adjust follow_patterns to match your site's URL structure
#   4. Run: webowui validate --site yoursite
#   5. Run: webowui scrape --site yoursite
#
# For production use, see:
#   - mediawiki.yml.example for MediaWiki sites
#   - example_site.yml.example for all available options
#   - README.md in this directory for detailed docs
# ============================================================================