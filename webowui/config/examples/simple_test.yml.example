# ============================================================================
# SIMPLE TEST CONFIGURATION
# ============================================================================
#
# This is a minimal example for testing and learning web-to-openwebui.
# It demonstrates the bare minimum configuration required to scrape a site.
#
# USAGE:
#   1. Copy this file: cp simple_test.yml.example mysite.yaml
#   2. Edit the URLs and patterns below
#   3. Test: webowui validate --site mysite
#   4. Scrape: webowui scrape --site mysite
#
# For more advanced features, see:
#   - mediawiki.yml.example (optimized for MediaWiki sites)
#   - example_site.yml.example (comprehensive reference)
#
# ============================================================================

# ----------------------------------------------------------------------------
# SITE INFORMATION (Required)
# ----------------------------------------------------------------------------
site:
  # Internal identifier - lowercase, no spaces, no special characters
  # This is how you reference the site in CLI commands
  name: "simple_test"

  # Human-readable display name (appears in logs and reports)
  display_name: "Simple Test Site"

  # Base URL - the root domain of the site you're scraping
  # All URLs must start with this base URL
  base_url: "https://example.com"

  # Starting URLs - where the scraper begins
  # These must be full URLs (not relative paths)
  start_urls:
    - "https://example.com/page"

# ----------------------------------------------------------------------------
# CRAWLING STRATEGY (Required)
# ----------------------------------------------------------------------------
crawling:
  # Strategy type - how the scraper follows links
  # Options: "bfs" (breadth-first), "dfs" (depth-first), "best_first"
  strategy: "bfs"

  # Maximum depth - how many link levels to follow from start URLs
  # depth=0: Only scrape start_urls
  # depth=1: Scrape start_urls + direct links from them
  # depth=2: Go one level deeper, etc.
  # Start with 1 for testing!
  max_depth: 1

  # URL Filters
  filters:
    # Follow patterns - regex patterns matching URLs to scrape
    # Use regex escape: \. for literal dots
    follow_patterns:
      - "^https://example\\.com/.*"

    # Exclude patterns - URLs matching these will NOT be scraped
    # Even if they match follow_patterns
    exclude_patterns:
      - ".*\\.pdf$"          # Skip PDF files
      - ".*\\.jpg$"          # Skip images
      - ".*\\?.*"            # Skip URLs with query parameters

  # Rate Limiting
  rate_limit:
    # Requests per second - be respectful to the server!
    # 1-2 req/sec is safe for most public sites
    requests_per_second: 1

    # Delay between requests (in seconds)
    # Additional throttling on top of requests_per_second
    delay_between_requests: 0.5

# ----------------------------------------------------------------------------
# CONTENT PROCESSING PIPELINE
# ----------------------------------------------------------------------------
# The content processing pipeline consists of 3 main stages:
#
# Stage 1: HTML Filtering (html_filtering)
#   - Applied to raw HTML before conversion
#   - Removes tags, links, and low-density blocks
#
# Stage 2: Markdown Processing
#   - 2a: Conversion (markdown_conversion) - HTML to Markdown
#   - 2b: Cleaning (markdown_cleaning) - Profile-based cleanup
#
# Stage 3: Result Filtering (result_filtering)
#   - Final checks on the generated markdown

# ----------------------------------------------------------------------------
# STAGE 1: HTML Filtering
# ----------------------------------------------------------------------------
html_filtering:
  # Heuristic Pruning Filter (aggressive content removal)
  # RECOMMENDED: enabled: false (rely on explicit tags and Stage 2 profiles instead)
  pruning:
    enabled: false
    # threshold: 0.6           # Content density (0.0-1.0)
    # min_word_threshold: 50   # Skip blocks with < N words

  # Basic HTML Filters (Always active if defined)
  # ---------------------------------------------

  # HTML tags to remove (standard HTML tag names ONLY, no classes/IDs)
  # excluded_tags: [nav, footer, aside, header]

  # Link filtering
  # exclude_external_links: true      # Remove links to external sites
  # exclude_social_media_links: true  # Remove social media share buttons

  # Block filtering (crawl4ai word_count_threshold)
  # Remove HTML blocks with fewer than N words
  min_block_words: 10

# ----------------------------------------------------------------------------
# STAGE 2a: Markdown Conversion
# ----------------------------------------------------------------------------
# Controls how crawl4ai converts HTML to markdown
markdown_conversion:
  # Main content selector (body gets everything, or use specific selector)
  content_selector: "body"

  # Elements to remove before extraction (CSS selectors supported here)
  # remove_selectors:
  #   - "script"     # JavaScript code
  #   - "style"      # CSS styles
  #   - "nav"        # Navigation menus
  #   - "footer"     # Page footers

  # Markdown generation options
  markdown_options:
    include_images: true       # Keep image references
    include_links: true        # Keep hyperlinks
    preserve_structure: true   # Maintain heading hierarchy
    heading_style: "atx"       # Use # for headings (ATX style)

# ----------------------------------------------------------------------------
# STAGE 2b: Markdown Cleaning
# ----------------------------------------------------------------------------
# Applied AFTER markdown generation to handle site-specific patterns
markdown_cleaning:
  # Profile: "mediawiki" (recommended), "none" (no cleaning), or custom
  # This selects the Cleaning Profile class to use (e.g., MediaWikiProfile)
  profile: "none"

  # Profile-specific options (see cleaning_profiles README for details)
  # config:
  #   remove_infoboxes: true

# ----------------------------------------------------------------------------
# STAGE 3: Result Filtering
# ----------------------------------------------------------------------------
# Final checks on the generated content
result_filtering:
  # Minimum page length (characters) - filter out stubs/redirects
  min_page_length: 100

  # Maximum page length (characters) - prevent memory issues
  max_page_length: 500000

  # Only scrape HTML pages
  allowed_content_types:
    - "text/html"

# ============================================================================
# THAT'S IT! This is the minimum configuration needed.
#
# NEXT STEPS:
#   1. Save this file with .yaml extension (not .yml.example)
#   2. Edit the base_url and start_urls above
#   3. Adjust follow_patterns to match your site's URL structure
#   4. Run: webowui validate --site yoursite
#   5. Run: webowui scrape --site yoursite
#
# For production use, see:
#   - mediawiki.yml.example for MediaWiki sites
#   - example_site.yml.example for all available options
#   - README.md in this directory for detailed docs
# ============================================================================
