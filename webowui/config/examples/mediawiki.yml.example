# =============================================================================
# MediaWiki Site Configuration Template
# =============================================================================
# This template is designed for MediaWiki-based sites. Copy this file to
# data/config/sites/yoursite.yaml and customize for your wiki.
#
# Tested with: Monster Hunter Wiki, Path of Exile 2 Wiki, Wikipedia
# Compatible with: Any MediaWiki installation (v1.35+)
#
# QUICK START:
#   1. Copy: cp mediawiki.yml.example mysite.yaml
#   2. Edit: Change base_url, start_urls, and site name
#   3. Test: webowui validate --site mysite
#   4. Scrape: webowui scrape --site mysite
# =============================================================================

# -----------------------------------------------------------------------------
# Site Identification
# -----------------------------------------------------------------------------
site:
  # Internal identifier (lowercase, no spaces, used for directory names)
  name: "mywiki"

  # Human-readable name (used in logs and OpenWebUI knowledge base)
  display_name: "My Wiki"

  # Base URL of the wiki (without /wiki/ path)
  base_url: "https://example.com"

  # Starting pages for the scraper (usually Main Page or index)
  start_urls:
    - "https://example.com/wiki/Main_Page"
    # Add more start URLs if needed:
    # - "https://example.com/wiki/Category:Important_Articles"

# -----------------------------------------------------------------------------
# Crawling Strategy
# -----------------------------------------------------------------------------
crawling:
  # Strategy: "bfs" (breadth-first, recommended), "dfs" (depth-first),
  #           or "best_first" (keyword-based)
  strategy: "bfs"

  # Maximum depth from start URLs (1 = only pages linked from start page)
  # Increase for comprehensive scraping, decrease for testing
  max_depth: 3

  # Optional: Limit total pages scraped
  max_pages: null

  # Optional: Stream results as they are crawled (vs batch at end)
  streaming: false

  # URL Filters
  filters:
    # URL patterns to follow (Python regex)
    follow_patterns:
      - "^https://example\\.com/wiki/.*"

    # URL patterns to exclude (Python regex)
    # These are standard MediaWiki pages you usually don't want
    exclude_patterns:
      - ".*Special:.*"        # Special pages (search, random, etc.)
      - ".*User:.*"           # User pages and profiles
      - ".*User_talk:.*"      # User discussion pages
      - ".*Talk:.*"           # Article talk/discussion pages
      - ".*File:.*"           # File description pages
      - ".*Template:.*"       # Wiki templates
      - ".*Category:.*"       # Category pages
      - ".*Help:.*"           # Help pages
      - ".*MediaWiki:.*"      # MediaWiki system pages
      - ".*action=edit.*"     # Edit pages
      - ".*action=history.*"  # History pages
      - ".*oldid=.*"          # Old revisions
      - ".*diff=.*"           # Difference pages
      - ".*printable=.*"      # Print versions

      # Wiki administrative/meta pages (about, policies, disclaimers)
      - ".*[Ww]iki:[Aa]bout"                 # About/info pages
      - ".*[Ww]iki:.*[Dd]isclaimer"          # Legal disclaimers
      - ".*[Ww]iki:[Cc]opyrights?"           # Copyright info
      - ".*[Ww]iki:[Pp]rivacy"               # Privacy policies
      - ".*[Ww]iki:[Cc]ommunity"             # Community guidelines

      # Edit/action pages and redirects
      - ".*/edit(\\?.*)?$"                   # Edit interface pages
      - ".*&action=(edit|history|delete|protect)"  # Action parameter pages
      - ".*\\?redlink="                      # Redirect/non-existent pages

      # Version history and changelog pages (optional - comment out if needed)
      - ".*[Vv]ersion.*[Hh]istory"           # Version history pages
      - ".*[Cc]hangelog"                     # Changelog pages
      - ".*[Pp]atch.*[Nn]otes"               # Patch notes pages

      # Image and file description pages
      - ".*Image:.*"          # Image description pages

    # Block specific domains (if crawling external links)
    exclude_domains: []

  # Rate limiting (be respectful to the server)
  rate_limit:
    requests_per_second: 1      # 1 request per second (conservative)
    delay_between_requests: 1.0  # Additional 1 second delay
    max_retries: 3               # Retry failed requests 3 times

# -----------------------------------------------------------------------------
# Content Filtering (Stage 1: HTML)
# -----------------------------------------------------------------------------
content_filtering:
  # Enable aggressive HTML filtering before markdown conversion
  enabled: false

  # Content density threshold (0.0 - 1.0)
  threshold: 0.6

  # Skip blocks with fewer than N words
  min_word_threshold: 50

  # Remove specific HTML tags
  excluded_tags:
    - "nav"
    - "footer"
    - "aside"
    - "script"
    - "style"

  # Exclude external links from content
  exclude_external_links: false

  # Exclude social media links
  exclude_social_media: false

# -----------------------------------------------------------------------------
# Content Extraction (Stage 2: Markdown)
# -----------------------------------------------------------------------------
extraction:
  # Main content selector (body gets everything, or use specific selector)
  content_selector: "body"

  # Elements to remove before extraction
  remove_selectors:
    - "script"     # JavaScript code
    - "style"      # CSS styles
    - "nav"        # Navigation menus
    - "footer"     # Page footers

  # Markdown generation options
  markdown_options:
    include_images: true       # Keep image references
    include_links: true        # Keep hyperlinks
    preserve_structure: true   # Maintain heading hierarchy
    heading_style: "atx"       # Use # for headings (ATX style)

# -----------------------------------------------------------------------------
# Content Filtering
# -----------------------------------------------------------------------------
filters:
  # Minimum content length (filter out navigation pages)
  min_content_length: 100

  # Maximum content length (prevent memory issues)
  max_content_length: 500000

  # Only scrape HTML pages
  allowed_content_types:
    - "text/html"

# -----------------------------------------------------------------------------
# Content Cleaning Profile
# -----------------------------------------------------------------------------
# The MediaWiki cleaning profile removes common wiki boilerplate to create
# embedding-ready content for RAG (Retrieval Augmented Generation)
cleaning:
  # Profile: "mediawiki" (recommended), "none" (no cleaning), or custom
  profile: "mediawiki"

  # Profile-specific configuration
  config:
    # Remove navigation elements (Jump to navigation/search links)
    remove_navigation_boilerplate: true

    # Remove wiki meta messages (work in progress banners, help text)
    remove_wiki_meta: true

    # Remove table of contents sections (auto-generated navigation)
    remove_table_of_contents: true

    # Remove infobox metadata tables (often at top of articles)
    remove_infoboxes: true

    # Remove "External Links" and "See also" sections
    remove_external_links: true

    # Remove version history and changelog sections
    remove_version_history: true

    # Remove template editing links ([v], [t], [e])
    remove_template_links: true

    # Remove citation markers and references
    remove_citations: true

    # Remove category listings
    remove_categories: true

    # Remove links to non-existent pages (reduce noise)
    filter_dead_links: true

# -----------------------------------------------------------------------------
# OpenWebUI Integration
# -----------------------------------------------------------------------------
openwebui:
  # Knowledge base name in OpenWebUI
  knowledge_name: "My Wiki"

  # Description shown in OpenWebUI
  description: "Comprehensive MediaWiki knowledge base for RAG"

  # Optional: Specify existing knowledge base ID to update (leave blank for new)
  # knowledge_id: "existing-kb-id-here"

  # Automatically upload after scraping
  auto_upload: false

  # Number of files to upload concurrently
  batch_size: 10

  # File deletion behavior when files are removed from scrape:
  # false = Delete file from storage (default, recommended)
  # true = Remove from knowledge but keep file in storage
  preserve_deleted_files: false

  # Auto-rebuild upload_status.json if missing (disaster recovery)
  auto_rebuild_state: true

  # Minimum confidence for state reconstruction: "high", "medium", or "low"
  rebuild_confidence_threshold: "medium"

# -----------------------------------------------------------------------------
# Backup Retention
# -----------------------------------------------------------------------------
retention:
  # Enable automatic backup cleanup
  enabled: true

  # Number of timestamped backup directories to keep (not counting current/)
  # Example with keep_backups: 2
  #   outputs/mysite/
  #   ├── current/              ← Always kept (active state)
  #   ├── 2025-11-17_10-00-00/ ← Backup 1 (most recent)
  #   ├── 2025-11-16_10-00-00/ ← Backup 2
  #   └── 2025-11-15_10-00-00/ ← DELETE (exceeds limit)
  keep_backups: 2

  # Run cleanup automatically after each scrape
  auto_cleanup: true

# -----------------------------------------------------------------------------
# Scheduling (for Docker daemon mode)
# -----------------------------------------------------------------------------
schedule:
  # Enable scheduled scraping (useful for Docker deployment)
  enabled: true

  # Schedule type: "cron" (cron syntax) or "interval" (fixed intervals)
  type: "cron"

  # Cron expression (for type: cron)
  # Examples:
  #   "0 2 * * *"      = 2 AM daily
  #   "0 */6 * * *"    = Every 6 hours
  #   "0 0 * * 0"      = Weekly on Sunday midnight
  cron: "0 2 * * *"

  # Interval settings (for type: interval)
  # interval:
  #   hours: 6         = Every 6 hours
  #   minutes: 30      = Every 30 minutes

  # Timezone for scheduled runs
  timezone: "America/Los_Angeles"

  # Maximum time allowed for scrape before timeout
  timeout_minutes: 60

  # Retry settings for failed scrapes
  retry:
    enabled: true           # Retry on failure
    max_attempts: 3         # Try up to 3 times total
    delay_minutes: 15       # Wait 15 minutes between attempts

# =============================================================================
# COMMON MEDIAWIKI SITES
# =============================================================================
#
# Wikipedia:
#   base_url: "https://en.wikipedia.org"
#   follow_patterns: ["^https://en\\.wikipedia\\.org/wiki/.*"]
#
# Fandom Wikis:
#   base_url: "https://yourwiki.fandom.com"
#   follow_patterns: ["^https://yourwiki\\.fandom\\.com/wiki/.*"]
#   cleaning: profile: "fandomwiki"  # Use Fandom-specific profile
#
# Miraheze Wikis:
#   base_url: "https://yourwiki.miraheze.org"
#   follow_patterns: ["^https://yourwiki\\.miraheze\\.org/wiki/.*"]
#
# Self-hosted MediaWiki:
#   base_url: "https://wiki.yourcompany.com"
#   follow_patterns: ["^https://wiki\\.yourcompany\\.com/wiki/.*"]
#
# =============================================================================
# TIPS AND BEST PRACTICES
# =============================================================================
#
# 1. START SMALL: Test with max_depth: 1 first to verify patterns work
#
# 2. RATE LIMITING: Adjust based on server capacity and your relationship
#    - Public wikis: 1 req/sec (conservative)
#    - Your own wiki: 2-5 req/sec (faster)
#
# 3. EXCLUDE PATTERNS: Add site-specific patterns you discover during testing
#    Example: - ".*Index_of_.*" if wiki has many index pages
#
# 4. CLEANING PROFILE: The mediawiki profile is optimized for embedding quality
#    but you can disable specific cleaners if needed
#
# 5. SCHEDULING: For frequently-updated wikis, use shorter intervals
#    For stable wikis, weekly or monthly scraping is sufficient
#
# 6. TESTING: Always validate before first scrape:
#    webowui validate --site mysite
#
# 7. INCREMENTAL UPDATES: After first scrape, only changed pages are uploaded
#    to save API quota and processing time
#
# =============================================================================
