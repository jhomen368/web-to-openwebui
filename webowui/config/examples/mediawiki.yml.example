# =============================================================================
# MediaWiki Site Configuration Template
# =============================================================================
# This template is designed for MediaWiki-based sites. Copy this file to
# data/config/sites/yoursite.yaml and customize for your wiki.
#
# Tested with: Monster Hunter Wiki, Path of Exile 2 Wiki, Wikipedia
# Compatible with: Any MediaWiki installation (v1.35+)
#
# QUICK START:
#   1. Copy: cp mediawiki.yml.example mysite.yaml
#   2. Edit: Change base_url, start_urls, and site name
#   3. Test: webowui validate --site mysite
#   4. Scrape: webowui scrape --site mysite
# =============================================================================

# -----------------------------------------------------------------------------
# Site Identification
# -----------------------------------------------------------------------------
site:
  # Internal identifier (lowercase, no spaces, used for directory names)
  name: "mywiki"

  # Human-readable name (used in logs and OpenWebUI knowledge base)
  display_name: "My Wiki"

  # Base URL of the wiki (without /wiki/ path)
  base_url: "https://example.com"

  # Starting pages for the scraper (usually Main Page or index)
  start_urls:
    - "https://example.com/wiki/Main_Page"
    # Add more start URLs if needed:
    # - "https://example.com/wiki/Category:Important_Articles"

# -----------------------------------------------------------------------------
# Crawling Strategy
# -----------------------------------------------------------------------------
crawling:
  # Strategy: "bfs" (breadth-first, recommended), "dfs" (depth-first),
  #           or "best_first" (keyword-based)
  strategy: "bfs"

  # Maximum depth from start URLs (1 = only pages linked from start page)
  # Increase for comprehensive scraping, decrease for testing
  max_depth: 3

  # Optional: Limit total pages scraped
  max_pages: null

  # Optional: Stream results as they are crawled (vs batch at end)
  streaming: false

  # Optional: Timeout for individual page loads (milliseconds)
  # Default: 60000 (60 seconds)
  # page_timeout: 60000

  # OPTIONAL: Wait condition before extracting content
  # CSS selector to wait for (ensures page is fully loaded)
  # Useful for sites with heavy JS or dynamic content
  # Default: null
  # wait_for: null

  # URL Filters
  filters:
    # URL patterns to follow (Python regex)
    follow_patterns:
      - "^https://example\\.com/wiki/.*"

    # URL patterns to exclude (Python regex)
    # These are standard MediaWiki pages you usually don't want
    exclude_patterns:
      - ".*Special:.*"        # Special pages (search, random, etc.)
      - ".*User:.*"           # User pages and profiles
      - ".*User_talk:.*"      # User discussion pages
      - ".*Talk:.*"           # Article talk/discussion pages
      - ".*File:.*"           # File description pages
      - ".*Template:.*"       # Wiki templates
      - ".*Category:.*"       # Category pages
      - ".*Help:.*"           # Help pages
      - ".*MediaWiki:.*"      # MediaWiki system pages
      - ".*action=edit.*"     # Edit pages
      - ".*action=history.*"  # History pages
      - ".*oldid=.*"          # Old revisions
      - ".*diff=.*"           # Difference pages
      - ".*printable=.*"      # Print versions

      # Wiki administrative/meta pages (about, policies, disclaimers)
      - ".*[Ww]iki:[Aa]bout"                 # About/info pages
      - ".*[Ww]iki:.*[Dd]isclaimer"          # Legal disclaimers
      - ".*[Ww]iki:[Cc]opyrights?"           # Copyright info
      - ".*[Ww]iki:[Pp]rivacy"               # Privacy policies
      - ".*[Ww]iki:[Cc]ommunity"             # Community guidelines

      # Edit/action pages and redirects
      - ".*/edit(\\?.*)?$"                   # Edit interface pages
      - ".*&action=(edit|history|delete|protect)"  # Action parameter pages
      - ".*\\?redlink="                      # Redirect/non-existent pages

      # Version history and changelog pages (optional - comment out if needed)
      - ".*[Vv]ersion.*[Hh]istory"           # Version history pages
      - ".*[Cc]hangelog"                     # Changelog pages
      - ".*[Pp]atch.*[Nn]otes"               # Patch notes pages

      # Image and file description pages
      - ".*Image:.*"          # Image description pages

    # Block specific domains (if crawling external links)
    exclude_domains: []

  # Rate limiting (be respectful to the server)
  rate_limit:
    requests_per_second: 1      # 1 request per second (conservative)
    delay_between_requests: 1.0  # Additional 1 second delay

# -----------------------------------------------------------------------------
# CONTENT PROCESSING PIPELINE
# -----------------------------------------------------------------------------
# The content processing pipeline consists of 3 main stages:
#
# Stage 1: HTML Filtering (html_filtering)
#   - Applied to raw HTML before conversion
#   - Removes tags, links, and low-density blocks
#
# Stage 2: Markdown Processing
#   - 2a: Conversion (markdown_conversion) - HTML to Markdown
#   - 2b: Cleaning (markdown_cleaning) - Profile-based cleanup
#
# Stage 3: Result Filtering (result_filtering)
#   - Final checks on the generated markdown

# -----------------------------------------------------------------------------
# STAGE 1: HTML Filtering
# -----------------------------------------------------------------------------
html_filtering:
  # Heuristic Pruning Filter (aggressive content removal)
  # RECOMMENDED: enabled: false (rely on explicit tags and Stage 2 profiles instead)
  pruning:
    enabled: false
    # threshold: 0.6           # Content density (0.0-1.0)
    # min_word_threshold: 50   # Skip blocks with < N words

  # Basic HTML Filters (Always active if defined)
  # ---------------------------------------------

  # HTML tags to remove (standard HTML tag names ONLY, no classes/IDs)
  # Optimal Safe Set for MediaWiki (Tested on Monster Hunter & PoE2 Wikis):
  excluded_tags:
    - nav
    - footer
    - aside
    - header

  # Link filtering
  exclude_external_links: true      # Remove links to external sites
  exclude_social_media_links: true  # Remove social media share buttons

  # Block filtering (crawl4ai word_count_threshold)
  # Remove HTML blocks with fewer than N words
  # Set to 10 to prevent aggressive filtering of short but valid content
  min_block_words: 10

# -----------------------------------------------------------------------------
# STAGE 2a: Markdown Conversion
# -----------------------------------------------------------------------------
# Controls how crawl4ai converts HTML to markdown
markdown_conversion:
  # Main content selector (body gets everything, or use specific selector)
  content_selector: "body"

  # Elements to remove before extraction (CSS selectors supported here)
  remove_selectors:
    - "script"     # JavaScript code
    - "style"      # CSS styles
    - "nav"        # Navigation menus
    - "footer"     # Page footers

  # Markdown generation options
  markdown_options:
    include_images: true       # Keep image references
    include_links: true        # Keep hyperlinks
    preserve_structure: true   # Maintain heading hierarchy
    heading_style: "atx"       # Use # for headings (ATX style)

# -----------------------------------------------------------------------------
# STAGE 2b: Markdown Cleaning (Profiles)
# -----------------------------------------------------------------------------
# Applied AFTER markdown generation to handle site-specific patterns
# The MediaWiki cleaning profile removes common wiki boilerplate to create
# embedding-ready content for RAG (Retrieval Augmented Generation)
markdown_cleaning:
  # Profile: "mediawiki" (recommended), "none" (no cleaning), or custom
  # This selects the Cleaning Profile class to use (e.g., MediaWikiProfile)
  profile: "mediawiki"

  # Profile-specific options (see cleaning_profiles README for details)
  # These defaults are optimized for standard MediaWiki sites
  config:
    # Core Wiki Cleaning
    remove_infoboxes: true          # Remove metadata tables at top
    remove_table_of_contents: true  # Remove auto-generated TOC
    remove_external_links: true     # Remove "External links" sections
    remove_version_history: true    # Remove changelogs/history tables
    remove_wiki_meta: true          # Remove "Work in progress" banners
    remove_navigation_boilerplate: true # Remove "Jump to navigation"
    remove_template_links: true     # Remove [v] [t] [e] links
    remove_media: true              # Gallery/Images/Videos sections
    remove_references_section: true # References/Notes/Footnotes
    remove_header_navigation: true  # Top navigation (Search, etc.)

    # Custom Header Patterns (Regex list)
    # Add site-specific header patterns to remove here
    # custom_header_patterns:
    #   - "^#\\s+About\\s+MyWiki\\s*$"
    #   - "^To\\s+learn\\s+how\\s+to\\s+contribute.*$"

    # Optional Filtering
    remove_citations: true          # Remove [1][2] markers
    remove_categories: true         # Remove category links at bottom
    filter_dead_links: false        # Remove links to non-existent pages

# -----------------------------------------------------------------------------
# STAGE 3: Result Filtering
# -----------------------------------------------------------------------------
# Final checks on the generated content
result_filtering:
  # Minimum page length (characters) - filter out stubs/redirects
  min_page_length: 100

  # Maximum page length (characters) - prevent memory issues
  max_page_length: 500000

# -----------------------------------------------------------------------------
# OpenWebUI Integration
# -----------------------------------------------------------------------------
openwebui:
  # Knowledge base name in OpenWebUI
  knowledge_name: "My Wiki"

  # Description shown in OpenWebUI
  description: "Comprehensive MediaWiki knowledge base for RAG"

  # Optional: Specify existing knowledge base ID to update (leave blank for new)
  # knowledge_id: "existing-kb-id-here"

  # Automatically upload after scraping
  auto_upload: false

  # Number of files to upload concurrently
  batch_size: 10

  # File deletion behavior when files are removed from scrape:
  # false = Delete file from storage (default, recommended)
  # true = Remove from knowledge but keep file in storage
  preserve_deleted_files: false

  # Auto-rebuild upload_status.json if missing (disaster recovery)
  auto_rebuild_state: true

  # Minimum confidence for state reconstruction: "high", "medium", or "low"
  rebuild_confidence_threshold: "medium"

# -----------------------------------------------------------------------------
# Backup Retention
# -----------------------------------------------------------------------------
retention:
  # Enable automatic backup cleanup
  enabled: true

  # Number of timestamped backup directories to keep (not counting current/)
  # Example with keep_backups: 2
  #   outputs/mysite/
  #   ├── current/              ← Always kept (active state)
  #   ├── 2025-11-17_10-00-00/ ← Backup 1 (most recent)
  #   ├── 2025-11-16_10-00-00/ ← Backup 2
  #   └── 2025-11-15_10-00-00/ ← DELETE (exceeds limit)
  keep_backups: 2

  # Run cleanup automatically after each scrape
  auto_cleanup: true

# -----------------------------------------------------------------------------
# Scheduling (for Docker daemon mode)
# -----------------------------------------------------------------------------
schedule:
  # Enable scheduled scraping (useful for Docker deployment)
  enabled: true

  # Schedule type: "cron" (cron syntax) or "interval" (fixed intervals)
  type: "cron"

  # Cron expression (for type: cron)
  # Examples:
  #   "0 2 * * *"      = 2 AM daily
  #   "0 */6 * * *"    = Every 6 hours
  #   "0 0 * * 0"      = Weekly on Sunday midnight
  cron: "0 2 * * *"

  # Interval settings (for type: interval)
  # interval:
  #   hours: 6         = Every 6 hours
  #   minutes: 30      = Every 30 minutes

  # Timezone for scheduled runs
  timezone: "America/Los_Angeles"

  # Maximum time allowed for scrape before timeout
  timeout_minutes: 60

  # Retry settings for failed scrapes
  retry:
    enabled: true           # Retry on failure
    max_attempts: 3         # Try up to 3 times total
    delay_minutes: 15       # Wait 15 minutes between attempts

# =============================================================================
# COMMON MEDIAWIKI SITES
# =============================================================================
#
# Wikipedia:
#   base_url: "https://en.wikipedia.org"
#   follow_patterns: ["^https://en\\.wikipedia\\.org/wiki/.*"]
#
# Fandom Wikis:
#   base_url: "https://yourwiki.fandom.com"
#   follow_patterns: ["^https://yourwiki\\.fandom\\.com/wiki/.*"]
#   markdown_cleaning: profile: "fandomwiki"  # Use Fandom-specific profile
#
# Miraheze Wikis:
#   base_url: "https://yourwiki.miraheze.org"
#   follow_patterns: ["^https://yourwiki\\.miraheze\\.org/wiki/.*"]
#
# Self-hosted MediaWiki:
#   base_url: "https://wiki.yourcompany.com"
#   follow_patterns: ["^https://wiki\\.yourcompany\\.com/wiki/.*"]
#
# =============================================================================
# TIPS AND BEST PRACTICES
# =============================================================================
#
# 1. START SMALL: Test with max_depth: 1 first to verify patterns work
#
# 2. RATE LIMITING: Adjust based on server capacity and your relationship
#    - Public wikis: 1 req/sec (conservative)
#    - Your own wiki: 2-5 req/sec (faster)
#
# 3. EXCLUDE PATTERNS: Add site-specific patterns you discover during testing
#    Example: - ".*Index_of_.*" if wiki has many index pages
#
# 4. CLEANING PROFILE: The mediawiki profile is optimized for embedding quality
#    but you can disable specific cleaners if needed
#
# 5. SCHEDULING: For frequently-updated wikis, use shorter intervals
#    For stable wikis, weekly or monthly scraping is sufficient
#
# 6. TESTING: Always validate before first scrape:
#    webowui validate --site mysite
#
# 7. INCREMENTAL UPDATES: After first scrape, only changed pages are uploaded
#    to save API quota and processing time
#
# =============================================================================
