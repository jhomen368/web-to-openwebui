# =============================================================================
# Fandom Wiki Site Configuration Template
# =============================================================================
# This template is designed for Fandom-hosted wikis (formerly Wikia).
# Copy this file to data/config/sites/yoursite.yaml and customize.
#
# Tested with: Escape from Tarkov Wiki (Fandom)
# Compatible with: Any wiki hosted on fandom.com
#
# QUICK START:
#   1. Copy: cp fandomwiki.yml.example myfandom.yaml
#   2. Edit: Change base_url, start_urls, and site name
#   3. Test: webowui validate --site myfandom
#   4. Scrape: webowui scrape --site myfandom
# =============================================================================

# -----------------------------------------------------------------------------
# Site Identification
# -----------------------------------------------------------------------------
site:
  # Internal identifier (lowercase, no spaces, used for directory names)
  name: "myfandom"

  # Human-readable name (used in logs and OpenWebUI knowledge base)
  display_name: "My Fandom Wiki"

  # Base URL of the wiki (without /wiki/ path)
  base_url: "https://yourwiki.fandom.com"

  # Starting pages for the scraper (usually Main Page or index)
  start_urls:
    - "https://yourwiki.fandom.com/wiki/Main_Page"
    # Add more start URLs if needed:
    # - "https://yourwiki.fandom.com/wiki/Category:Important_Articles"

# -----------------------------------------------------------------------------
# Crawling Strategy
# -----------------------------------------------------------------------------
crawling:
  # Strategy: "bfs" (breadth-first, recommended), "dfs" (depth-first),
  #           or "best_first" (keyword-based)
  strategy: "bfs"

  # Maximum depth from start URLs (1 = only pages linked from start page)
  # Increase for comprehensive scraping, decrease for testing
  max_depth: 3

  # Optional: Limit total pages scraped
  max_pages: null

  # Optional: Stream results as they are crawled (vs batch at end)
  streaming: false

  # URL Filters
  filters:
    # URL patterns to follow (Python regex)
    follow_patterns:
      - "^https://yourwiki\\.fandom\\.com/wiki/.*"

    # URL patterns to exclude (Python regex)
    # These are standard MediaWiki pages you usually don't want
    exclude_patterns:
      - ".*Special:.*"        # Special pages (search, random, etc.)
      - ".*User:.*"           # User pages and profiles
      - ".*User_talk:.*"      # User discussion pages
      - ".*Talk:.*"           # Article talk/discussion pages
      - ".*File:.*"           # File description pages
      - ".*Template:.*"       # Wiki templates
      - ".*Category:.*"       # Category pages
      - ".*Help:.*"           # Help pages
      - ".*MediaWiki:.*"      # MediaWiki system pages
      - ".*action=edit.*"     # Edit pages
      - ".*action=history.*"  # History pages
      - ".*oldid=.*"          # Old revisions
      - ".*diff=.*"           # Difference pages
      - ".*printable=.*"      # Print versions
      - ".*Blog:.*"           # Fandom blog posts
      - ".*Message_Wall:.*"   # Fandom message walls
      - ".*Forum:.*"          # Fandom forums

      # Wiki administrative/meta pages (about, policies, disclaimers)
      - ".*[Ww]iki:[Aa]bout"                 # About/info pages
      - ".*[Ww]iki:.*[Dd]isclaimer"          # Legal disclaimers
      - ".*[Ww]iki:[Cc]opyrights?"           # Copyright info
      - ".*[Ww]iki:[Pp]rivacy"               # Privacy policies
      - ".*[Ww]iki:[Cc]ommunity"             # Community guidelines

      # Edit/action pages and redirects
      - ".*/edit(\\?.*)?$"                   # Edit interface pages
      - ".*&action=(edit|history|delete|protect)"  # Action parameter pages
      - ".*\\?redlink="                      # Redirect/non-existent pages

      # Version history and changelog pages (optional - comment out if needed)
      - ".*[Vv]ersion.*[Hh]istory"           # Version history pages
      - ".*[Cc]hangelog"                     # Changelog pages
      - ".*[Pp]atch.*[Nn]otes"               # Patch notes pages

      # Image and file description pages
      - ".*Image:.*"          # Image description pages

    # Block specific domains (if crawling external links)
    exclude_domains: []

  # Rate limiting (be respectful to the server)
  rate_limit:
    requests_per_second: 1      # 1 request per second (conservative)
    delay_between_requests: 1.0  # Additional 1 second delay
    max_retries: 3               # Retry failed requests 3 times

# -----------------------------------------------------------------------------
# CONTENT PROCESSING PIPELINE
# -----------------------------------------------------------------------------
# The content processing pipeline consists of 3 main stages:
#
# Stage 1: HTML Filtering (html_filtering)
#   - Applied to raw HTML before conversion
#   - Removes tags, links, and low-density blocks
#
# Stage 2: Markdown Processing
#   - 2a: Conversion (markdown_conversion) - HTML to Markdown
#   - 2b: Cleaning (markdown_cleaning) - Profile-based cleanup
#
# Stage 3: Result Filtering (result_filtering)
#   - Final checks on the generated markdown

# -----------------------------------------------------------------------------
# STAGE 1: HTML Filtering
# -----------------------------------------------------------------------------
html_filtering:
  # Heuristic Pruning Filter (aggressive content removal)
  # RECOMMENDED: enabled: false (rely on explicit tags and Stage 2 profiles instead)
  pruning:
    enabled: false
    # threshold: 0.6           # Content density (0.0-1.0)
    # min_word_threshold: 50   # Skip blocks with < N words

  # Basic HTML Filters (Always active if defined)
  # ---------------------------------------------

  # HTML tags to remove (standard HTML tag names ONLY, no classes/IDs)
  # Optimal Safe Set for Fandom (Tested on Escape from Tarkov Wiki):
  excluded_tags:
    - nav
    - footer
    - aside
    - header

  # Link filtering
  exclude_external_links: true      # Remove links to external sites
  exclude_social_media_links: true  # Remove social media share buttons

  # Block filtering (crawl4ai word_count_threshold)
  # Remove HTML blocks with fewer than N words
  # Set to 10 to prevent aggressive filtering of short but valid content
  min_block_words: 10

# -----------------------------------------------------------------------------
# STAGE 2a: Markdown Conversion
# -----------------------------------------------------------------------------
# Controls how crawl4ai converts HTML to markdown
markdown_conversion:
  # Main content selector (body gets everything, or use specific selector)
  content_selector: "body"

  # Elements to remove before extraction (CSS selectors supported here)
  remove_selectors:
    - "script"     # JavaScript code
    - "style"      # CSS styles
    - "nav"        # Navigation menus
    - "footer"     # Page footers
    - ".wds-global-footer" # Fandom global footer
    - ".global-navigation" # Fandom global navigation
    - ".fandom-community-header" # Fandom community header
    - ".page-header__contribution-buttons" # Edit buttons
    - ".page-header__categories" # Categories (if you want to remove them early)
    - "#mixed-content-footer" # "More Fandom" footer

  # Markdown generation options
  markdown_options:
    include_images: true       # Keep image references
    include_links: true        # Keep hyperlinks
    preserve_structure: true   # Maintain heading hierarchy
    heading_style: "atx"       # Use # for headings (ATX style)

# -----------------------------------------------------------------------------
# STAGE 2b: Markdown Cleaning (Profiles)
# -----------------------------------------------------------------------------
# Applied AFTER markdown generation to handle site-specific patterns
# The FandomWiki cleaning profile extends MediaWiki cleaning with Fandom-specific
# logic to remove ads, promotions, and community boilerplate.
markdown_cleaning:
  # Profile: "fandomwiki" (recommended for Fandom sites)
  # This selects the FandomWikiProfile class
  profile: "fandomwiki"

  # Profile-specific options (see cleaning_profiles README for details)
  # These defaults are optimized for Fandom sites
  config:
    # Fandom-Specific Cleaning
    remove_fandom_ads: true         # Remove "Advertisement" markers
    remove_fandom_promotions: true  # Remove "More Fandom", "Fan Central"
    remove_community_content: true  # Remove community feed/discussions
    remove_related_wikis: true      # Remove "Explore other wikis"
    remove_fandom_footer: true      # Remove global footer navigation

    # Core Wiki Cleaning (Inherited from MediaWiki)
    remove_infoboxes: true          # Remove metadata tables at top
    remove_table_of_contents: true  # Remove auto-generated TOC
    remove_external_links: true     # Remove "External links" sections
    remove_version_history: true    # Remove changelogs/history tables
    remove_wiki_meta: true          # Remove "Work in progress" banners
    remove_navigation_boilerplate: true # Remove "Jump to navigation"
    remove_template_links: true     # Remove [v] [t] [e] links

    # NEW: Additional Noise Removal
    remove_media: true              # Gallery/Images/Videos sections
    remove_references_section: true # References/Notes/Footnotes
    remove_header_navigation: true  # Top navigation (Search, etc.)

    # Optional Filtering
    remove_citations: true          # Remove [1][2] markers
    remove_categories: true         # Remove category links at bottom
    filter_dead_links: false        # Remove links to non-existent pages

# -----------------------------------------------------------------------------
# STAGE 3: Result Filtering
# -----------------------------------------------------------------------------
# Final checks on the generated content
result_filtering:
  # Minimum page length (characters) - filter out stubs/redirects
  min_page_length: 100

  # Maximum page length (characters) - prevent memory issues
  max_page_length: 500000

  # Only scrape HTML pages
  allowed_content_types:
    - "text/html"

# -----------------------------------------------------------------------------
# OpenWebUI Integration
# -----------------------------------------------------------------------------
openwebui:
  # Knowledge base name in OpenWebUI
  knowledge_name: "My Fandom Wiki"

  # Description shown in OpenWebUI
  description: "Comprehensive Fandom knowledge base for RAG"

  # Optional: Specify existing knowledge base ID to update (leave blank for new)
  # knowledge_id: "existing-kb-id-here"

  # Automatically upload after scraping
  auto_upload: false

  # Number of files to upload concurrently
  batch_size: 10

  # File deletion behavior when files are removed from scrape:
  # false = Delete file from storage (default, recommended)
  # true = Remove from knowledge but keep file in storage
  preserve_deleted_files: false

  # Auto-rebuild upload_status.json if missing (disaster recovery)
  auto_rebuild_state: true

  # Minimum confidence for state reconstruction: "high", "medium", or "low"
  rebuild_confidence_threshold: "medium"

# -----------------------------------------------------------------------------
# Backup Retention
# -----------------------------------------------------------------------------
retention:
  # Enable automatic backup cleanup
  enabled: true

  # Number of timestamped backup directories to keep (not counting current/)
  keep_backups: 2

  # Run cleanup automatically after each scrape
  auto_cleanup: true

# -----------------------------------------------------------------------------
# Scheduling (for Docker daemon mode)
# -----------------------------------------------------------------------------
schedule:
  # Enable scheduled scraping (useful for Docker deployment)
  enabled: true

  # Schedule type: "cron" (cron syntax) or "interval" (fixed intervals)
  type: "cron"

  # Cron expression (for type: cron)
  # Examples:
  #   "0 2 * * *"      = 2 AM daily
  #   "0 */6 * * *"    = Every 6 hours
  #   "0 0 * * 0"      = Weekly on Sunday midnight
  cron: "0 2 * * *"

  # Timezone for scheduled runs
  timezone: "America/Los_Angeles"

  # Maximum time allowed for scrape before timeout
  timeout_minutes: 60

  # Retry settings for failed scrapes
  retry:
    enabled: true           # Retry on failure
    max_attempts: 3         # Try up to 3 times total
    delay_minutes: 15       # Wait 15 minutes between attempts

# =============================================================================
# TIPS AND BEST PRACTICES
# =============================================================================
#
# 1. FANDOM SPECIFICS: Fandom sites are heavy on JavaScript and ads.
#    The combination of html_filtering (Stage 1) and the fandomwiki profile
#    (Stage 2) is essential to get clean content.
#
# 2. RATE LIMITING: Fandom servers can be sensitive. Stick to 1 req/sec.
#
# 3. EXCLUDE PATTERNS: Fandom has many community pages (User:, Talk:, Message_Wall:)
#    that are not useful for RAG. The default exclude_patterns handle most of these.
#
# 4. CLEANING PROFILE: The fandomwiki profile extends the mediawiki profile.
#    It inherits all MediaWiki cleaning logic and adds Fandom-specific rules.
#
# 5. TESTING: Always validate before first scrape:
#    webowui validate --site myfandom
#
# =============================================================================
